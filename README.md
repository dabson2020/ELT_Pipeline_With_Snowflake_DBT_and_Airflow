## Snowflake Data Engineering Project – ELT Pipeline with dbt & Airflow Project Overview

This project implements a comprehensive data engineering pipeline that utilizes Snowflake as the data warehouse, dbt for data transformation and testing, GitHub for version control, and Airflow for orchestration.

The pipeline ingests raw data into Snowflake, transforms it into business-ready datasets using dbt, and schedules automated runs using Airflow. The entire process is version-controlled with GitHub for collaborative development and CI/CD.

### Business Requirement

The business requirement is to analyze 
- Customer Revenue: Customer Segment based on Revenue
- Employee Performance: Top employee based on revenue and total orders
- Product Performance: Compute the revenue generated by each product
- Quarterly Target: Based on the Sales target, the revenue was computed to determine the percentage achieved
- Customer History: Here, the customer history is updated using the snapshot materialization in dbt, which updates customers' information with Slow Changing Dimension. 
- 
The organization’s datasets include:

- Customers
- Orders
- OrderItems
- Stores
- Employees
- Products
- Suppliers
- Dates
- SalesTargets

The goal is to:

- Consolidate raw transactional data.
- Create a fact table for orders.
- Create the computation discussed above
- Store the results in a consumption layer for analytics and reporting.

### DATA ARCHITECTURE
The data warehouse utilized for this project is Snowflake. In Sq=nowflake, the database, schema, virtual warehouse, roles, and users are created
In the database, three (3) schemas are created, namely Landing, Processing, and Consumption, which imitate the medallion architecture.

- #### LANDING
Tables are created to ingest the data. Data are ingested from various sources, which include XML, JSON, CSV, REST APIS, and other databases. In other words, structured and semi-structured data are ingested into the data warehouse. Snowflake can read semi-structured data like structured data by identifying semi-structured data as a VARIANT data type. Data are ingested into the following tables:
- Customers
- Orders
- OrderItems
- Stores
- Employees
- Products
- Suppliers
- Dates
- SalesTargets

The staging data is created to stage the data. These are created as views in the LANDING schema. These staged data are referenced (created as Jinja constructs) and reusable, enabling manageability and testability.

#### PROCESSING – Transformed datasets including fact tables.

Example: order_facts table created from order_stg and orderitems_stg.
Customer_orders is created from customer_stg and orders_stg

These tables are referenced and reusable when creating the computations to meet business requirements.


#### CONSUMPTION – Business-ready datasets for BI & analytics.

The business requirements are discussed above.

### ELT PROCESS

Extract & Load (EL)
Source Data: Customer, order, and product-related datasets are ingested into Snowflake’s landing schema.

Data ingestion was performed via 
- bulk load (Snowpipe)
- COPY INTO with SQL
- INSERT STATEMENTS with SQL

Transform (T) with dbt
Transformation logic is handled by dbt, following the modular SQL approach

a. Staging Layer (staging folder)

Each raw table in landing is transformed into a cleaned staging table with standardized column names, data types, and filters.

Example:

order_stg – cleans and standardizes order data.

orderitems_stg – cleans and standardizes order item data.

customer_stg – cleans and standardizes customer data.

b. Fact Table Creation

order_facts table is built by joining order_stg and orderitems_stg.

Metrics calculated:

Order revenue (unit price × quantity).

Order count per customer.

c. Business Computations

order_facts is joined with customer_stg to compute:

Top 10 customers by revenue.

Top 10 customers by number of orders.

The results are stored in the consumption schema.

dbt Features Used

source.yml – Defines all source tables and their relationships.

Macros & Jinja – Reusable SQL logic for revenue calculation and customer ranking.

Tests:

Singular tests – Validate specific business rules.

Generic tests – Check for not_null, unique, and accepted_values.

Snapshots – Implements Slowly Changing Dimension Type 2 (SCD2) to track historical changes in customer data.

Future scope: Implement Azure DevOps/GitHub Actions for automated deployment on commit to main.

![alt text](<dags/dbt/sleet_ols_project/lineage graph.png>)

Orchestration with Airflow
Airflow DAG schedules dbt runs to execute transformations daily.

DAG Tasks:

Check data availability in Snowflake landing tables.

Run dbt models (dbt run).

Run dbt tests (dbt test).

Load results into consumption schema.

The pipeline is monitored and logged in Airflow UI.

Version Control & CI/CD

GitHub repository stores dbt project code, including models, macros, and tests.

Branching strategy ensures feature development is merged into main only after review.

Key Benefits of This Pipeline

Separation of concerns – Raw, processing, and consumption schemas clearly isolate data states.

Reproducibility – dbt’s SQL-based transformations ensure transparent logic.

Scalability – Airflow handles automated orchestration and scheduling.

Data Quality – dbt tests and snapshots maintain accuracy over time.

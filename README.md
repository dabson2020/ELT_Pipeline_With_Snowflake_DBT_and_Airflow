## Snowflake Data Engineering Project – ELT Pipeline with dbt & Airflow Project Overview

This project implements a comprehensive data engineering pipeline that utilizes Snowflake as the data warehouse, dbt for data transformation and testing, GitHub for version control, and Airflow for orchestration.

The pipeline ingests raw data into Snowflake, transforms it into business-ready datasets using dbt, and schedules automated runs using Airflow. The entire process is version-controlled with GitHub for collaborative development and CI/CD.

### Business Requirement

The business requirements is to analyze 
- Customer Revenue: Customer Segment based on Revenue
- Employee Performance: Top employee based on revenue and total orders
- Product Performance: Compute the revenue generated by each product
- Quarterly Target: Based on the Sales target, the revenue was computed to determine the percentage achieved
- Customer History: Here, the customer history is updated using the snapshot materialization in dbt, which updates customers' information with Slow Changing Dimension. 
- 
The organization’s datasets include:

- Customers
- Orders
- OrderItems
- Stores
- Employees
- Products
- Suppliers
- Dates
- SalesTargets

The goal is to:

- Consolidate raw transactional data.
- Create a fact table for orders.
- Create the computation discussed above
- Store the results in a consumption layer for analytics and reporting.

### DATA ARCHITECTURE
The data warehouse utilized for this project is Snowflake. In Snowflake, the database, schema, virtual warehouse, roles, and users are created
In the database, three (3) schemas are created, namely Landing, Processing, and Consumption, which imitate the medallion architecture.

- #### LANDING
Tables are created to ingest the data. Data are ingested from various sources, which include XML, JSON, CSV, REST APIS, and other databases. In other words, structured and semi-structured data are ingested into the data warehouse. Snowflake can read semi-structured data like structured data by identifying semi-structured data as a VARIANT data type. Data are ingested into the following tables:
- Customers
- Orders
- OrderItems
- Stores
- Employees
- Products
- Suppliers
- Dates
- SalesTargets

The staging data is created to stage the data. These are created as views in the LANDING schema. These staged data are referenced (created as Jinja constructs) and reusable, enabling manageability and testability.

#### PROCESSING – Transformed datasets including fact tables.

Example: order_facts table created from order_stg and orderitems_stg.
Customer_orders is created from customer_stg and orders_stg

These tables are referenced and reusable when creating the computations to meet business requirements.


#### CONSUMPTION – Business-ready datasets for BI & analytics.

The business requirements are discussed above.

### ELT PROCESS

#### Extract & Load (EL)
Source Data: 
Customers, Orders, OrderItems, Stores, Employees, Products, Suppliers, Dates and SalesTargets datasets are ingested into Snowflake’s landing schema.

Data ingestion was performed via 
- bulk load (Snowpipe)
- COPY INTO with SQL
- INSERT STATEMENTS with SQL

#### Transform (T) with dbt
Transformation logic is handled by dbt, following the modular SQL approach
- Macros: Nacros are used to encapsulate complex logic. They are reusable functions across multiple models. These we utilised to create functions that were utilized in the transformation models.
- Jinja: These are utilized to make SQL and YAML dynamic

##### - Staging Layer (staging folder)

Each raw table in landing is transformed into a cleaned staging table with standardized column names, data types, and filters. Here, column name changes are effected and the required columns are selected for each table

Example:

All the tables in landing were cleaned and standardized and materialized as views for the stage data. The staged data were loaded in the Landing schema.

##### - Processing Layer (Processing Folder)

order_facts table is built by joining order_stg and orderitems_stg.
Customer_orders table is built by joinging customer_stg and orders_stg.
These tables are referenced in the consumption layer 



##### - Business Computations (Consumption Layer)


- Top 10 customers by revenue and total number of orders. The customer are segmented into "high Value', middle value and 'low value' based on the revenue generated. The high value customers are the top 20% revenue makers, low value customers are the low 20% customer.

- Top 10 employees by revenue and order numbers

- Top Store performance products by revenue

- Quarterly target and percentage achieved.


The results are stored in the consumption schema.

### DBT FEATURES

source.yml – Defines all source tables and their relationships.

Macros & Jinja – Reusable SQL logic for revenue calculation and customer ranking.

#### Tests:

 -Singular tests – Validate specific business rules.

 - Generic tests – Check for not_null, unique, and accepted_values. Generic test were also created to meet some business requirements. For example, Ensuring the Unit price column is less than 0. A generic test is created under Macros folder and implemented to test the column. If there is any value less than 0, the test will fail which will help to resolve thie issue

### MATERIALIZATION
Snapshots – Implemented Slowly Changing Dimension Type 2 (SCD2) to track historical changes in customer data. This was utilized when configuring the materialization of the table. 
Materialization can be configured are
 - view
 - tables
 - incremental : These are utilized when you want to load only the incremental values into the table
 - ephemeral: The table is not materialized in the data warehouse but subsequent models that references it will be materialized.
 - snapshots: It implements slowing changing data Type 2. This keeps the old record and create a new row for the new record, adding the update date.

Future scope: Implement Azure DevOps/GitHub Actions for automated deployment on commit to main.

### DBT DOCUMENTATION
Dbt provide the opportunity to add documentation while creating the models. The benfits includes:
- Improve communication amongst stakeholders
- Make project understandable and maintainable
- Accelerated new team members onboarding

The documentation created are implemented after running 'dbt docs generate'. To share with others, 'dbt docs serve' is ran. This provide the DBT documentation as a hyperlink. The link provides all the documentation and information about the database, schemas, tables and the custom documentation added to the models. A lineage graph is also generate to visualize the dependencies and the relationships with the tables 
Below is the Lineage graph of the dbt transformation for this project

![alt text](<dags/dbt/sleet_ols_project/lineage graph.png>)

### Orchestration with Airflow
Airflow DAG schedules dbt runs to execute transformations daily.
Astromoer is installed and initialized. This creates dockerfile what warehouse the codes and models
In the requirements/txt file, the required packages are installed. The project is copied into the dags of the airflow and the astro is started eith 'sudo astro dev start. This starts the airflow as well which opens on port 8080 by default. Here, the prok=ject is show as a dag that is run based on the schedule. In this project, the dag is ran daily.

DAG Tasks:

- Check data availability in Snowflake landing tables.

- Run dbt models (dbt run).

- Run dbt tests (dbt test).

- Create and load the staging data

- Compute and load the processing folder.

- Load results into consumption schema.

- The pipeline is monitored and logged in Airflow UI.

Below is the airflow graph for the project
![alt text](<dags/dbt/sleet_ols_project/dbt sleet project img.jpg>)

### Version Control & CI/CD

GitHub repository stores dbt project code, including models, macros, and tests.

Branching strategy ensures feature development is merged into main only after review.

### Key Benefits of This Pipeline

Separation of concerns – Raw, processing, and consumption schemas clearly isolate data states.

Reproducibility – dbt’s SQL-based transformations ensure transparent logic.

Scalability – Airflow handles automated orchestration and scheduling.

Data Quality – dbt tests and snapshots maintain accuracy over time.
